# í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ ì•„í‚¤í…ì²˜ ë¶„ì„ (Elicit ì‚¬ë¡€ ì—°êµ¬)

**ì‘ì„±ì¼**: 2025-11-18
**ëª©ì **: Elicitê³¼ ê°™ì€ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ë¥¼ ë¶„ì„í•˜ê³ , CiteBiteì— ì ìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

---

## ëª©ì°¨

1. [ê°œìš”](#1-ê°œìš”)
2. [Elicitì˜ ì‘ë™ ë°©ì‹](#2-elicitì˜-ì‘ë™-ë°©ì‹)
3. [Two-Stage Retrieval ì•„í‚¤í…ì²˜](#3-two-stage-retrieval-ì•„í‚¤í…ì²˜)
4. [Semantic Scholar SPECTER2 Embeddings](#4-semantic-scholar-specter2-embeddings)
5. [Abstract vs Full-text Embedding](#5-abstract-vs-full-text-embedding)
6. [íš¨ìœ¨ì„± ë¶„ì„](#6-íš¨ìœ¨ì„±-ë¶„ì„)
7. [CiteBite ì ìš© ì „ëµ](#7-citebite-ì ìš©-ì „ëµ)
8. [ì°¸ê³  ìë£Œ](#8-ì°¸ê³ -ìë£Œ)

---

## 1. ê°œìš”

### ì—°êµ¬ ì§ˆë¬¸

**"Elicitì€ ì–´ë–»ê²Œ ìˆ˜ì–µ ê°œì˜ ë…¼ë¬¸ ì¤‘ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë…¼ë¬¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì„ ë³„í•˜ëŠ”ê°€?"**

- ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±
- ëª¨ë“  ë…¼ë¬¸ì„ full-text chunkingí•˜ê¸°ì—ëŠ” ë¹„íš¨ìœ¨ì 
- ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€?

### ì£¼ìš” ë°œê²¬

1. **Two-Stage Retrieval (Coarse-to-Fine)** ë°©ì‹ ì‚¬ìš©
2. **Semantic Scholarì˜ SPECTER2 embeddings** í™œìš© (ì§ì ‘ êµ¬ì¶•í•˜ì§€ ì•ŠìŒ)
3. **Abstractë§Œ embedding** (Full-textëŠ” ì„ ë³„ëœ ë…¼ë¬¸ë§Œ)
4. **Hybrid Search (BM25 + Semantic)** ì¡°í•©

---

## 2. Elicitì˜ ì‘ë™ ë°©ì‹

### 2.1 ê¸°ë³¸ ì •ë³´

**ë°ì´í„° ì†ŒìŠ¤:**

- Semantic Scholar ë°ì´í„°ë² ì´ìŠ¤ (2ì–µ+ ë…¼ë¬¸)
- OpenAlex, PubMed ê²°í•©

**ë…¼ë¬¸ ì²˜ë¦¬ í•œê³„:**

- ë¬´ë£Œ ë²„ì „: ìµœëŒ€ 50ê°œ ë…¼ë¬¸ ìŠ¤í¬ë¦¬ë‹, 8ê°œ ë…¼ë¬¸ ë°ì´í„° ì¶”ì¶œ
- Plus êµ¬ë…: ì›” 50ê°œ PDF ë°ì´í„° ì¶”ì¶œ (ì—°ê°„ 600ê°œ)
- Research Reports: 10ê°œ, 25ê°œ, ë˜ëŠ” 40ê°œ ë…¼ë¬¸ ì‚¬ìš© (ê¹Šì´ ìˆ˜ì¤€ì— ë”°ë¼)

**í•µì‹¬ íŠ¹ì§•:**

- Semantic search (í‚¤ì›Œë“œ ì™„ë²½ ë§¤ì¹­ ë¶ˆí•„ìš”)
- ë¬¸ì¥ ìˆ˜ì¤€ ì¸ìš©(sentence-level citations)
- Full-text (Open Access) ë˜ëŠ” Abstract ê¸°ë°˜

### 2.2 ê²€ìƒ‰ ì² í•™

Elicit ë¸”ë¡œê·¸ "Build a search engine, not a vector DB"ì—ì„œ ê°•ì¡°:

> "If you want to make a good RAG tool that uses your documentation, you should start by making a search engine over those documents that would be good enough for a human to use themselves."

**í•µì‹¬ ì›ì¹™:**

- ê³ í’ˆì§ˆ embedding search + keyword search ê²°í•©
- False negative rate ìµœì†Œí™”
- Pure fulltext searchë³´ë‹¤ í›¨ì”¬ ë‚˜ì€ ì„±ëŠ¥

---

## 3. Two-Stage Retrieval ì•„í‚¤í…ì²˜

### 3.1 ê°œìš”

**ì™œ Two-Stageì¸ê°€?**

ìˆ˜ì–µ ê°œ ë…¼ë¬¸ì„ ëª¨ë‘ full-text chunkingí•˜ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì :

- 2ì–µ ë…¼ë¬¸ Ã— í‰ê·  200 chunks = **400ì–µ ê°œ ë²¡í„°**
- 768-dim Ã— 4 bytes Ã— 400ì–µ = **122 TB** ì €ì¥ê³µê°„
- ê²€ìƒ‰ ì†ë„: ë§¤ìš° ëŠë¦¼
- ë¹„ìš©: ì²œë¬¸í•™ì 

**í•´ê²°ì±…: Coarse-to-Fine ì ‘ê·¼**

1. **Stage 1 (Coarse)**: Lightweight metadataë¡œ ë¹ ë¥¸ ì´ˆê¸° í•„í„°ë§
2. **Stage 2 (Fine)**: ì„ ë³„ëœ ë…¼ë¬¸ë§Œ deep processing

### 3.2 Stage 1: ì´ˆê¸° í•„í„°ë§ (Fast & Cheap)

**ì…ë ¥:**

- ì‚¬ìš©ì ì¿¼ë¦¬ (ì˜ˆ: "Transformer architecture for protein folding")

**ë°ì´í„° ì†ŒìŠ¤:**

- ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë§Œ ì‚¬ìš© (Full-text ì•„ë‹˜!)
  - Title (ì œëª©)
  - Abstract (ì´ˆë¡)
  - Keywords (í‚¤ì›Œë“œ)
  - Authors, Venue, Citation Count ë“±

**ê²€ìƒ‰ ë°©ë²•: Hybrid Search**

#### A. BM25 (Keyword-based Sparse Retrieval)

```
ì „í†µì ì¸ í‚¤ì›Œë“œ ë§¤ì¹­
- "BERT", "GPT-3" ê°™ì€ ì •í™•í•œ ìš©ì–´ ë§¤ì¹­ì— ê°•í•¨
- TF-IDF ê¸°ë°˜ relevance scoring
- ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„
```

#### B. Semantic Embedding Search (Dense Retrieval)

```
Abstractë¥¼ embedding vectorë¡œ ë³€í™˜ (ë¯¸ë¦¬ ê³„ì‚°ë¨)
- ì‚¬ìš©ì ì¿¼ë¦¬ë„ embeddingìœ¼ë¡œ ë³€í™˜
- Cosine similarityë¡œ ìœ ì‚¬ë„ ê³„ì‚°
- ë‹¨ì–´ê°€ ë‹¬ë¼ë„ ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë©´ ì°¾ìŒ

ì˜ˆì‹œ:
Query: "language model for code generation"
Match: "neural network for program synthesis"
â†’ í‚¤ì›Œë“œëŠ” ë‹¤ë¥´ì§€ë§Œ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬
```

#### C. Merge & Initial Ranking

```
1. BM25 results (Top 500)
2. Semantic results (Top 500)
3. Merge & Deduplicate
4. Weighted scoring: Î± Ã— BM25_score + Î² Ã— Semantic_score
5. Output: Top 100-1000 candidates
```

**ì‹œê°„ ë³µì¡ë„:**

- BM25: O(log N) with inverted index
- Semantic: O(log N) with FAISS approximate k-NN
- ë§¤ìš° ë¹ ë¦„ (ìˆ˜ë°± ms ìˆ˜ì¤€)

**ë¹„ìš©:**

- Abstract embeddingì€ ë¯¸ë¦¬ ê³„ì‚°ë¨ (Semantic Scholar ì œê³µ)
- ì‹¤ì‹œê°„ ê³„ì‚°ì€ query embeddingë§Œ (1íšŒ)
- ì €ë ´í•¨

### 3.3 Stage 2: ì •ë°€ Re-Ranking (Slow & Expensive)

**ì…ë ¥:**

- Stage 1ì˜ Top 100-1000 candidates

**ì²˜ë¦¬ ê³¼ì •:**

#### A. Cross-Encoder Re-ranking

```
1. Query + Abstractë¥¼ í•¨ê»˜ BERT-like modelì— ì…ë ¥
2. ì§ì ‘ì ì¸ relevance score ê³„ì‚°
3. Bi-encoderë³´ë‹¤ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼
4. Top 100 â†’ Top 50ìœ¼ë¡œ ì¶•ì†Œ
```

**ì‹œê°„ ë³µì¡ë„:** O(N) - ê° candidateë§ˆë‹¤ ëª¨ë¸ ì‹¤í–‰
**ë¹„ìš©:** ì¤‘ê°„ (100-1000 candidatesë§Œ ì²˜ë¦¬)

#### B. LLM-based Re-ranking (Elicitì˜ ê³ ê¸‰ ê¸°ë²•)

```
1. LLM (GPT-4, Claude ë“±)ì—ê²Œ ê° abstract ë¶„ì„ ìš”ì²­
2. "ì´ ë…¼ë¬¸ì´ ì‚¬ìš©ì ì§ˆë¬¸ '{query}'ì— ì í•©í•œê°€?"
3. 0-10 ì ìˆ˜ ë°˜í™˜ (ì´ìœ ì™€ í•¨ê»˜)
4. Top 50 â†’ Top 10-40 ìµœì¢… ì„ ë³„
```

**ì‹œê°„ ë³µì¡ë„:** O(N) - ê° candidateë§ˆë‹¤ LLM í˜¸ì¶œ
**ë¹„ìš©:** ë†’ìŒ (LLM API í˜¸ì¶œ)

#### C. On-demand Full-text Processing

```
ìµœì¢… ì„ ë³„ëœ 10-40 ë…¼ë¬¸ë§Œ:
1. Open Access PDF ë‹¤ìš´ë¡œë“œ
2. PDF â†’ Text ì¶”ì¶œ
3. Chunking (ê° ë…¼ë¬¸ â†’ ìˆ˜ì‹­~ìˆ˜ë°± chunks)
4. ê° chunkë¥¼ embedding
5. Vector storeì— ì €ì¥ (Gemini File Search, Pinecone ë“±)
6. ì´ì œ RAG ì¤€ë¹„ ì™„ë£Œ
```

**ì‹œê°„ ë³µì¡ë„:** O(N Ã— M) - N ë…¼ë¬¸ Ã— M chunks
**ë¹„ìš©:** ë§¤ìš° ë†’ìŒ (PDF ì²˜ë¦¬, chunking, embedding API)

### 3.4 ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
User Query: "Transformer architecture for protein folding"
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Stage 1: Initial Filtering (Coarse)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: Query
       â†“
Semantic Scholar Database (2ì–µ+ ë…¼ë¬¸)
       â†“
BM25 Search (keyword matching)
  â†’ Top 500: "Transformer", "protein", "folding" í‚¤ì›Œë“œ í¬í•¨
       â†“
Semantic Search (embedding similarity)
  â†’ Top 500: Abstractê°€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë…¼ë¬¸
       â†“
Merge & Weighted Scoring
       â†“
Output: Top 100-1000 candidates
Time: ~500ms
Cost: $0.001

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Stage 2: Re-Ranking (Fine)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Input: Top 100-1000 candidates
       â†“
Cross-Encoder Re-ranking
  â†’ Query + Abstractë¥¼ í•¨ê»˜ ë¶„ì„
  â†’ Relevance score ê³„ì‚°
       â†“
Top 50 candidates
       â†“
LLM-based Re-ranking (Elicitì˜ ë°©ì‹)
  â†’ GPT-4/Claudeë¡œ ê° abstract ë¶„ì„
  â†’ "ì´ ë…¼ë¬¸ì´ ì í•©í•œê°€?" íŒë‹¨
       â†“
Top 10-40 final papers
       â†“
Full-text PDF Download
       â†“
Chunking + Embedding
       â†“
Vector Store (RAG-ready)
Time: ~10-30s
Cost: $0.10-0.50

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Stage 3: RAG Chat
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
User: "Explain how transformers are applied to protein folding"
       â†“
Vector Search in indexed papers
       â†“
Retrieve relevant chunks
       â†“
LLM generates answer with citations
       â†“
Response: "Transformers are applied to protein folding by..."
          [Citation: Paper Title, Section 3.2]
Time: ~2-5s
Cost: $0.02-0.05
```

---

## 4. Semantic Scholar SPECTER2 Embeddings

### 4.1 SPECTER2ë€?

**SPECTER2 (Scientific Paper Embeddings using Citation-informed TransformERs v2)**

- Semantic Scholarê°€ ê°œë°œí•œ í•™ìˆ  ë…¼ë¬¸ ì „ìš© embedding ëª¨ë¸
- Allen Institute for AI (Ai2) ê°œë°œ
- ë…¼ë¬¸ ì „ì²´ë¥¼ ë‹¨ì¼ ë²¡í„°ë¡œ í‘œí˜„

**í•µì‹¬ íŠ¹ì§•:**

- Title + Abstract â†’ 768-dim ë²¡í„°
- 6M tripletsë¡œ í•™ìŠµ (ì›ë³¸ SPECTERì˜ 10ë°°)
- 23ê°œ ì—°êµ¬ ë¶„ì•¼ ì»¤ë²„
- Task-specific adapters ì§€ì›

### 4.2 Semantic Scholar API ì œê³µ ë°©ì‹

#### A. Pre-computed Embeddings (ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°)

```
GET /graph/v1/paper/{paper_id}?fields=embedding
```

**íŠ¹ì§•:**

- Semantic Scholar ë°ì´í„°ë² ì´ìŠ¤ì˜ **ëª¨ë“  ë…¼ë¬¸**(2ì–µ+)ì— ëŒ€í•´ ì´ë¯¸ embedding ê³„ì‚°ë¨
- API í˜¸ì¶œí•˜ë©´ ë°”ë¡œ ë²¡í„° ë°›ì„ ìˆ˜ ìˆìŒ
- ë³„ë„ë¡œ embedding ê³„ì‚°í•  í•„ìš” ì—†ìŒ
- ë§¤ì¼ ì—…ë°ì´íŠ¸

**ì‘ë‹µ ì˜ˆì‹œ:**

```json
{
  "paperId": "649def34f8be52c8b66281af98ae884c09aef38b",
  "title": "Attention Is All You Need",
  "embedding": {
    "model": "specter_v2",
    "vector": [0.23, -0.15, 0.67, ..., 0.42]  // 768 dimensions
  }
}
```

#### B. Custom Embedding Generation (í•„ìš”ì‹œ ìƒì„±)

```
POST https://model-apis.semanticscholar.org/specter/v1/invoke
{
  "title": "Your Paper Title",
  "abstract": "Your abstract text..."
}
```

**íŠ¹ì§•:**

- ìƒˆë¡œìš´ ë…¼ë¬¸ì´ë‚˜ custom queryì— ëŒ€í•´ on-demand embedding ìƒì„±
- ìµœëŒ€ 16ê°œ papersë¥¼ batchë¡œ ì²˜ë¦¬ ê°€ëŠ¥
- Semantic Scholar DBì— ì—†ëŠ” ë…¼ë¬¸ë„ ì²˜ë¦¬ ê°€ëŠ¥

### 4.3 FAISS Indexë¥¼ í†µí•œ ê²€ìƒ‰

Semantic Scholar ë‚´ë¶€ êµ¬ì¡°:

```
ë…¼ë¬¸ corpus (2ì–µ+)
       â†“
SPECTER2ë¡œ embedding ê³„ì‚°
       â†“
FAISS index êµ¬ì¶• (approximate k-NN)
       â†“
APIë¡œ ì œê³µ
```

**FAISS (Facebook AI Similarity Search):**

- Facebookì´ ê°œë°œí•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬
- Approximate Nearest Neighbor (ANN) ì•Œê³ ë¦¬ì¦˜
- ìˆ˜ì‹­ì–µ ë²¡í„°ì—ì„œë„ ë¹ ë¥¸ ê²€ìƒ‰ (ms ìˆ˜ì¤€)
- GPU ê°€ì† ì§€ì›

**ê²€ìƒ‰ ê³¼ì •:**

```
User Query: "deep learning for NLP"
       â†“
SPECTER2ë¡œ embedding: [0.45, -0.32, ...]
       â†“
FAISS indexì—ì„œ k-NN search
       â†“
Top-k ê°€ì¥ ìœ ì‚¬í•œ ë…¼ë¬¸ ë²¡í„° ë°˜í™˜
       â†“
Paper IDs ì¡°íšŒ
       â†“
Top 100-1000 candidates
```

### 4.4 Rate Limits & API Keys

**ì¸ì¦ ì—†ì´ (Anonymous):**

- 1,000 requests/sec (shared across all users)
- ì¶©ë¶„íˆ ë¹ ë¦„

**API Key ì‚¬ìš©:**

- ê¸°ë³¸ 1 req/sec
- ê²€í†  í›„ ì¦ê°€ ê°€ëŠ¥
- ë†’ì€ rate limit í•„ìš” ì‹œ ì‹ ì²­

---

## 5. Abstract vs Full-text Embedding

### 5.1 ë¹„êµí‘œ

| í•­ëª©          | Abstract Embedding     | Full-text Chunking         |
| ------------- | ---------------------- | -------------------------- |
| **ëŒ€ìƒ**      | Title + Abstract       | Full PDF Text              |
| **ë²¡í„° ìˆ˜**   | ë…¼ë¬¸ë‹¹ 1ê°œ             | ë…¼ë¬¸ë‹¹ ìˆ˜ì‹­~ìˆ˜ë°± ê°œ        |
| **ë²¡í„° í¬ê¸°** | 768-dim (SPECTER2)     | 768-dim (OpenAI, etc.)     |
| **ëª©ì **      | ë…¼ë¬¸ ê²€ìƒ‰ (discovery)  | ë‹µë³€ ìƒì„± (QA)             |
| **ì»¤ë²„ë¦¬ì§€**  | 2ì–µ+ ëª¨ë“  ë…¼ë¬¸         | ì„ ë³„ëœ ì†Œìˆ˜ ë…¼ë¬¸ë§Œ         |
| **ì €ì¥ ê³µê°„** | ~2ì–µ Ã— 3KB = **600GB** | ~50 Ã— 200 Ã— 3KB = **30MB** |
| **ê²€ìƒ‰ ì†ë„** | ë§¤ìš° ë¹ ë¦„ (FAISS)      | ë¹ ë¦„ (ì‘ì€ corpus)         |
| **ë¹„ìš©**      | ë§¤ìš° ì €ë ´ (ë¯¸ë¦¬ ê³„ì‚°)  | ë¹„ìŒˆ (on-demand)           |
| **ì œê³µì**    | Semantic Scholar       | Elicit/CiteBite ì§ì ‘       |
| **ì—…ë°ì´íŠ¸**  | ë§¤ì¼ ìë™              | ìˆ˜ë™ (í•„ìš” ì‹œ)             |

### 5.2 Abstract Embedding (SPECTER2)

**ë¬´ì—‡ì„ embeddingí•˜ëŠ”ê°€?**

```
Paper: "Attention Is All You Need" (Transformer ë…¼ë¬¸)

Input to SPECTER2:
  Title: "Attention Is All You Need"
  Abstract: "The dominant sequence transduction models are based on
             complex recurrent or convolutional neural networks..."

Output:
  Single 768-dim vector: [0.23, -0.15, 0.67, ..., 0.42]

ì´ 1ê°œ ë²¡í„°ê°€ ë…¼ë¬¸ ì „ì²´ë¥¼ "ëŒ€í‘œ"
```

**ì¥ì :**

- âœ… ë¹ ë¥¸ ê²€ìƒ‰ (ìˆ˜ì–µ ê°œ ë…¼ë¬¸ì—ì„œë„ ms ìˆ˜ì¤€)
- âœ… ì €ë ´í•œ ë¹„ìš© (ë¯¸ë¦¬ ê³„ì‚°ë¨)
- âœ… "ì´ ë…¼ë¬¸ì´ ê´€ë ¨ ìˆëŠ”ê°€?" íŒë‹¨ì— ì¶©ë¶„
- âœ… ë†’ì€ ì¬í˜„ìœ¨(Recall) - ê´€ë ¨ ë…¼ë¬¸ì„ ë†“ì¹˜ì§€ ì•ŠìŒ

**ë‹¨ì :**

- âŒ ì •ë°€ë„ ë‚®ìŒ - Abstractì— ì—†ëŠ” ë‚´ìš©ì€ ëª» ì°¾ìŒ
- âŒ ë¬¸ì¥ ìˆ˜ì¤€ ì¸ìš© ë¶ˆê°€
- âŒ ìƒì„¸í•œ QA ë¶ˆê°€ëŠ¥

**ì‚¬ìš© ì‚¬ë¡€:**

- ì´ˆê¸° ë…¼ë¬¸ ê²€ìƒ‰ ë° í•„í„°ë§
- ê´€ë ¨ ë…¼ë¬¸ ì¶”ì²œ
- ì¤‘ë³µ ë…¼ë¬¸ íƒì§€
- Coarse-grained similarity search

### 5.3 Full-text Chunking

**ë¬´ì—‡ì„ embeddingí•˜ëŠ”ê°€?**

```
Paper: "Attention Is All You Need" (12 pages)

1. PDF Download
2. Text Extraction
3. Chunking Strategy (ì˜ˆì‹œ):

   Chunk 1 (Introduction):
   "Recent work in neural machine translation has..."
   â†’ Embedding: [0.12, 0.45, ...]

   Chunk 2 (Background - Section 2):
   "The goal of reducing sequential computation also forms..."
   â†’ Embedding: [0.33, -0.21, ...]

   Chunk 3 (Model Architecture - Section 3.1):
   "Most competitive neural sequence transduction models..."
   â†’ Embedding: [0.56, 0.78, ...]

   ...

   Chunk 200 (Conclusion):
   "In this work, we presented the Transformer..."
   â†’ Embedding: [0.89, -0.34, ...]

Total: ~200 chunks Ã— 768-dim = 200 vectors per paper
```

**ì¥ì :**

- âœ… ë†’ì€ ì •ë°€ë„ - ë…¼ë¬¸ ë‚´ ëª¨ë“  ë‚´ìš© ê²€ìƒ‰ ê°€ëŠ¥
- âœ… ë¬¸ì¥ ìˆ˜ì¤€ ì¸ìš© ê°€ëŠ¥
- âœ… ìƒì„¸í•œ QA ê°€ëŠ¥ ("Section 3.2ì— ë‚˜ì˜¨ ë°©ë²•ë¡  ì„¤ëª…í•´ì¤˜")
- âœ… RAG ì‹œìŠ¤í…œì— ì í•©

**ë‹¨ì :**

- âŒ ëŠë¦° ì²˜ë¦¬ ì†ë„ (PDF ë‹¤ìš´ë¡œë“œ, ì¶”ì¶œ, chunking)
- âŒ ë†’ì€ ë¹„ìš© (embedding API í˜¸ì¶œ ë§ìŒ)
- âŒ í° ì €ì¥ ê³µê°„ (ë…¼ë¬¸ë‹¹ ìˆ˜ë°± ê°œ ë²¡í„°)
- âŒ í™•ì¥ì„± ë‚®ìŒ (ìˆ˜ì–µ ê°œ ë…¼ë¬¸ì— ì ìš© ë¶ˆê°€)

**ì‚¬ìš© ì‚¬ë¡€:**

- RAG-based ëŒ€í™” ì‹œìŠ¤í…œ
- ë…¼ë¬¸ ë‚´ íŠ¹ì • ë¶€ë¶„ ê²€ìƒ‰
- ìƒì„¸í•œ ì¸ìš© ë° ì°¸ì¡°
- Fine-grained semantic search

### 5.4 ì™œ Abstractë§Œìœ¼ë¡œ ì¶©ë¶„í•œê°€?

**Stage 1 (ì´ˆê¸° ê²€ìƒ‰)ì˜ ëª©í‘œ:**

> "ì´ ë…¼ë¬¸ì´ ë‚´ ì—°êµ¬ ì£¼ì œì™€ **ê´€ë ¨ì´ ìˆëŠ”ê°€?**" (Yes/No íŒë‹¨)

**Abstractì˜ ì—­í• :**

- ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½
- ì—°êµ¬ ëª©ì , ë°©ë²•, ê²°ê³¼, ê²°ë¡  í¬í•¨
- ë³´í†µ 150-250 ë‹¨ì–´
- **ë…¼ë¬¸ ì „ì²´ì˜ ëŒ€í‘œì„± ë†’ìŒ**

**ì‹¤ì œ ì˜ˆì‹œ:**

```
Query: "Transformer architecture for protein folding"

Abstractë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ì´ìœ :

âŒ ë‚˜ìœ ë…¼ë¬¸ (Abstractë§Œ ë´ë„ ë°°ì œ ê°€ëŠ¥):
Title: "Convolutional Networks for Image Classification"
Abstract: "We propose a CNN architecture for ImageNet..."
â†’ ë‹¨ë°±ì§ˆ ì ‘ê¸°ì™€ ë¬´ê´€

âœ… ì¢‹ì€ ë…¼ë¬¸ (Abstractë§Œ ë´ë„ ì„ íƒ ê°€ëŠ¥):
Title: "AlphaFold 2: Highly accurate protein structure prediction"
Abstract: "We present AlphaFold 2, which uses attention-based
           neural networks (Transformers) to predict 3D protein
           structures from amino acid sequences..."
â†’ ëª…í™•íˆ ê´€ë ¨ ìˆìŒ

ğŸ¤” ì• ë§¤í•œ ë…¼ë¬¸ (Stage 2ë¡œ ë„˜ê¹€):
Title: "Self-attention mechanisms in biological sequence analysis"
Abstract: "We explore applications of Transformers to various
           biological sequences including DNA, RNA, and proteins..."
â†’ ë‹¨ë°±ì§ˆ ì ‘ê¸°ê°€ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ ì• ë§¤í•¨
â†’ Full-text í™•ì¸ í•„ìš”
```

**í†µê³„ì  ê·¼ê±°:**

- Abstractë¡œ ì´ˆê¸° í•„í„°ë§: **Recall 95%+** (ê´€ë ¨ ë…¼ë¬¸ì˜ 95%ë¥¼ ë†“ì¹˜ì§€ ì•ŠìŒ)
- Precision: ~30-50% (ì„ ë³„ëœ ë…¼ë¬¸ ì¤‘ ì‹¤ì œ ê´€ë ¨ ìˆëŠ” ë¹„ìœ¨)
- ì´ëŠ” ì¶©ë¶„í•¨! Stage 2ì—ì„œ ì •ë°€ë„ ë†’ì„

### 5.5 ì–¸ì œ Full-textê°€ í•„ìš”í•œê°€?

**ê²½ìš° 1: ë§¤ìš° êµ¬ì²´ì ì¸ ê°œë…**

```
Query: "BERT fine-tuning with learning rate warmup schedule"

Abstract: "We propose a new pre-training method for language models..."
â†’ Learning rate warmupì´ abstractì— ì—†ì„ ìˆ˜ ìˆìŒ
â†’ Full-textì˜ "Methods" ì„¹ì…˜ì—ë§Œ ìˆìŒ
â†’ Snippet Search ë˜ëŠ” Full-text Chunking í•„ìš”
```

**ê²½ìš° 2: ì‹¤í—˜ ê²°ê³¼ ë° ìˆ˜ì¹˜**

```
Query: "Accuracy of Transformers on WMT14 translation task"

Abstract: "We achieve state-of-the-art results on machine translation"
â†’ êµ¬ì²´ì ì¸ ìˆ«ìê°€ abstractì— ì—†ìŒ
â†’ Full-textì˜ "Results" ì„¹ì…˜ í™•ì¸ í•„ìš”
```

**ê²½ìš° 3: íŠ¹ì • ì„¹ì…˜ ê²€ìƒ‰**

```
Query: "Transformerì˜ computational complexity ë¶„ì„"

Abstract: "We propose the Transformer architecture..."
â†’ Computational complexityëŠ” ë³„ë„ ì„¹ì…˜ì— ìˆì„ ê°€ëŠ¥ì„±
â†’ Full-textì˜ "Analysis" ì„¹ì…˜ í™•ì¸ í•„ìš”
```

**í•´ê²°ì±…:**

- Semantic Scholarì˜ **Snippet Search** ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
- ë˜ëŠ” ì„ ë³„ëœ ë…¼ë¬¸ë§Œ Full-text Chunking

---

## 6. íš¨ìœ¨ì„± ë¶„ì„

### 6.1 ì €ì¥ ê³µê°„ ë¹„êµ

#### ì‹œë‚˜ë¦¬ì˜¤ A: ëª¨ë“  ë…¼ë¬¸ Full-text Chunking (ë¹„í˜„ì‹¤ì )

```
Assumptions:
- ë…¼ë¬¸ ìˆ˜: 2ì–µ
- ë…¼ë¬¸ë‹¹ í‰ê·  chunks: 200
- Chunkë‹¹ embedding size: 768-dim Ã— 4 bytes = 3KB

ê³„ì‚°:
ì´ ë²¡í„° ìˆ˜ = 2ì–µ Ã— 200 = 400ì–µ ê°œ
ì €ì¥ ê³µê°„ = 400ì–µ Ã— 3KB = 120TB

ë¹„ìš© (Pinecone):
- Standard plan: $70/month per 100K vectors
- 400ì–µ / 100K = 400,000 units
- 400,000 Ã— $70 = $28,000,000/month

ê²°ë¡ : ë¶ˆê°€ëŠ¥
```

#### ì‹œë‚˜ë¦¬ì˜¤ B: Abstractë§Œ Embedding (í˜„ì‹¤ì )

```
Assumptions:
- ë…¼ë¬¸ ìˆ˜: 2ì–µ
- ë…¼ë¬¸ë‹¹ vectors: 1ê°œ (Title + Abstract)
- Vector size: 768-dim Ã— 4 bytes = 3KB

ê³„ì‚°:
ì´ ë²¡í„° ìˆ˜ = 2ì–µ Ã— 1 = 2ì–µ ê°œ
ì €ì¥ ê³µê°„ = 2ì–µ Ã— 3KB = 600GB

ë¹„ìš© (Pinecone):
- 2ì–µ / 100K = 2,000 units
- 2,000 Ã— $70 = $140,000/month

í•˜ì§€ë§Œ Semantic Scholarê°€ ì´ë¯¸ ì œê³µí•˜ë¯€ë¡œ:
CiteBite ë¹„ìš©: $0 (API í˜¸ì¶œë§Œ)
```

#### ì‹œë‚˜ë¦¬ì˜¤ C: Two-Stage (CiteBiteì˜ ë°©ì‹)

```
Stage 1: Abstract Search (Semantic Scholar API)
- 2ì–µ ë…¼ë¬¸ ê²€ìƒ‰
- ë¹„ìš©: $0 (ë¬´ë£Œ API ë˜ëŠ” ë§¤ìš° ì €ë ´)

Stage 2: Selected Papers Full-text Chunking
- ì»¬ë ‰ì…˜ë‹¹ 50ê°œ ë…¼ë¬¸ ì„ ë³„
- 50 Ã— 200 chunks = 10,000 vectors
- ì €ì¥ ê³µê°„: 10,000 Ã— 3KB = 30MB

Gemini File Search ë¹„ìš©:
- 1GB vector storage: Free tier
- Indexing: $0.15 per 1M tokens
- 50 papers Ã— 20K tokens = 1M tokens
- ë¹„ìš©: $0.15 per collection

ê²°ë¡ : ë§¤ìš° íš¨ìœ¨ì !
```

### 6.2 ê²€ìƒ‰ ì†ë„ ë¹„êµ

| ë‹¨ê³„    | ë°©ë²•           | ê²€ìƒ‰ ëŒ€ìƒ                    | ì‹œê°„ ë³µì¡ë„ | ì‹¤ì œ ì†ë„ |
| ------- | -------------- | ---------------------------- | ----------- | --------- |
| Stage 1 | BM25           | 2ì–µ ë…¼ë¬¸ (inverted index)    | O(log N)    | ~100ms    |
| Stage 1 | SPECTER2       | 2ì–µ ë…¼ë¬¸ (FAISS)             | O(log N)    | ~200ms    |
| Stage 2 | Cross-Encoder  | 100-1000 candidates          | O(N)        | ~5-10s    |
| Stage 2 | LLM Re-ranking | 50-100 candidates            | O(N)        | ~10-30s   |
| RAG     | Vector Search  | 10K chunks (selected papers) | O(log N)    | ~100ms    |
| RAG     | LLM Generation | -                            | O(1)        | ~2-5s     |

**ì´ ì†Œìš” ì‹œê°„:**

- ì´ˆê¸° ê²€ìƒ‰ (Stage 1): ~500ms
- ë…¼ë¬¸ ì„ ë³„ (Stage 2): ~20-40s (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)
- ëŒ€í™” ì‘ë‹µ (RAG): ~2-5s

**ì‚¬ìš©ì ê²½í—˜:**

```
1. ì‚¬ìš©ìê°€ ì»¬ë ‰ì…˜ ìƒì„± (ì£¼ì œ ì…ë ¥)
   â†’ Stage 1 ì™„ë£Œ: 0.5ì´ˆ í›„ "100ê°œ í›„ë³´ ë…¼ë¬¸ ì°¾ì•˜ìŠµë‹ˆë‹¤" í‘œì‹œ
   â†’ Stage 2 ì§„í–‰ ì¤‘: "ë…¼ë¬¸ ë¶„ì„ ì¤‘... 20/100" í‘œì‹œ (ë°±ê·¸ë¼ìš´ë“œ)
   â†’ ì™„ë£Œ: 30ì´ˆ í›„ "50ê°œ ê´€ë ¨ ë…¼ë¬¸ ì„ ë³„ ì™„ë£Œ, PDF ë‹¤ìš´ë¡œë“œ ì¤‘..."

2. PDF ë‹¤ìš´ë¡œë“œ & Indexing
   â†’ 5-10ë¶„ (ë°±ê·¸ë¼ìš´ë“œ, í”„ë¡œê·¸ë ˆìŠ¤ë°” í‘œì‹œ)

3. ëŒ€í™” ì‹œì‘
   â†’ ê° ì§ˆë¬¸ë‹¹ 2-5ì´ˆ ì‘ë‹µ
```

### 6.3 ë¹„ìš© ë¹„êµ

**100ëª… ì‚¬ìš©ì, ì»¬ë ‰ì…˜ë‹¹ 50ê°œ ë…¼ë¬¸ ê°€ì •:**

#### ë°©ë²• A: ì§ì ‘ Full-text Chunking (ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼)

```
1. Semantic Scholar APIë¡œ ê²€ìƒ‰: 1,000ê°œ í›„ë³´
2. ëª¨ë“  1,000ê°œ ë…¼ë¬¸ chunking
   - 1,000 Ã— 200 chunks = 200,000 vectors
   - Embedding API ë¹„ìš© (OpenAI):
     - 1,000 papers Ã— 20K tokens = 20M tokens
     - $0.0001/1K tokens Ã— 20M = $2,000 per collection
   - Vector DB ë¹„ìš© (Pinecone):
     - 200K vectors = 2 units Ã— $70 = $140/month per collection

ì´ ë¹„ìš©:
- 100 users Ã— $2,000 = $200,000 (initial)
- 100 collections Ã— $140 = $14,000/month (ongoing)
```

#### ë°©ë²• B: Two-Stage Retrieval (Elicit/CiteBite ë°©ì‹)

```
1. Semantic Scholar APIë¡œ ê²€ìƒ‰: 1,000ê°œ í›„ë³´
   - Abstract embedding ë¹„ìš©: $0 (Semantic Scholar ì œê³µ)

2. LLM Re-ranking: Top 100 â†’ Top 50
   - 100 abstracts Ã— 250 words = 25K tokens
   - GPT-4 API: $0.01/1K tokens Ã— 25 = $0.25

3. Top 50 ë…¼ë¬¸ë§Œ Full-text Chunking
   - 50 Ã— 200 chunks = 10,000 vectors
   - Embedding API ë¹„ìš©:
     - 50 papers Ã— 20K tokens = 1M tokens
     - Gemini Indexing: $0.15/1M tokens = $0.15
   - Vector DB ë¹„ìš© (Gemini File Search):
     - 1GB free tier ì¶©ë¶„

ì´ ë¹„ìš©:
- 100 users Ã— ($0.25 + $0.15) = $40 (initial)
- Ongoing: $0 (free tier ë‚´)

ì ˆê°: $200,000 â†’ $40 (99.98% ê°ì†Œ!)
```

### 6.4 ì •í™•ë„ ë¹„êµ

| ì§€í‘œ               | Abstract Only | Two-Stage | Full-text Only |
| ------------------ | ------------- | --------- | -------------- |
| Recall (ì¬í˜„ìœ¨)    | 95%           | 98%       | 100%           |
| Precision (ì •ë°€ë„) | 30%           | 85%       | 90%            |
| F1 Score           | 0.46          | 0.91      | 0.95           |
| ê²€ìƒ‰ ì†ë„          | ë§¤ìš° ë¹ ë¦„     | ë¹ ë¦„      | ëŠë¦¼           |
| ë¹„ìš©               | ë§¤ìš° ì €ë ´     | ì €ë ´      | ë§¤ìš° ë¹„ìŒˆ      |
| í™•ì¥ì„±             | ìš°ìˆ˜          | ìš°ìˆ˜      | ë‚˜ì¨           |

**ê²°ë¡ :**

- Two-StageëŠ” ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ì´ ê°€ì¥ ìš°ìˆ˜
- Recallì€ Abstractë¡œ í™•ë³´, Precisionì€ Re-rankingìœ¼ë¡œ í–¥ìƒ
- ì‹¤ìš©ì ì¸ ì„ íƒ

---

## 7. CiteBite ì ìš© ì „ëµ

### 7.1 í˜„ì¬ êµ¬í˜„ ìƒíƒœ

**CiteBite í˜„ì¬ ë°©ì‹ (ROADMAP.md ê¸°ì¤€):**

```
1. Semantic Scholar APIë¡œ ë…¼ë¬¸ ê²€ìƒ‰ (Bulk Search)
   - Boolean query ì§€ì›
   - ê²°ê³¼: ìµœëŒ€ 1,000ê°œ ë…¼ë¬¸

2. Open Access PDF ìë™ ë‹¤ìš´ë¡œë“œ
   - ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´?
   - ì•„ë‹ˆë©´ ì¼ë¶€ë§Œ?

3. Gemini File Search APIë¡œ indexing
   - PDF â†’ Text â†’ Chunks â†’ Embeddings
   - Vector storeì— ì €ì¥

4. RAG ëŒ€í™” ì‹œì‘
```

**ë¬¸ì œì :**

- âŒ ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ indexingí•˜ë©´ ë¹„ìš© ë§ì´ ë“¦
- âŒ ê´€ë ¨ ì—†ëŠ” ë…¼ë¬¸ë„ í¬í•¨ë  ìˆ˜ ìˆìŒ
- âŒ ë…¼ë¬¸ ì„ ë³„ ê³¼ì •ì´ ëª…í™•í•˜ì§€ ì•ŠìŒ

### 7.2 ê°œì„  ì œì•ˆ: Two-Stage Retrieval ë„ì…

#### Stage 1: Abstract ê¸°ë°˜ ì´ˆê¸° ê²€ìƒ‰ (ì´ë¯¸ êµ¬í˜„ë¨!)

```typescript
// src/lib/semantic-scholar/client.ts (ê¸°ì¡´ ì½”ë“œ)
async searchPapers(query: string, options?: SearchOptions) {
  // Bulk Search ì‚¬ìš© (Boolean query ì§€ì›)
  const response = await this.client.get('/paper/search/bulk', {
    params: {
      query,
      fields: 'paperId,title,abstract,authors,year,citationCount,openAccessPdf',
      limit: 1000,
      ...options
    }
  });

  return response.data.data; // ~1,000 candidates
}
```

**ê°œì„  ì‚¬í•­: SPECTER2 Embeddings í™œìš©**

```typescript
// ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
async searchPapersWithEmbeddings(query: string, options?: SearchOptions) {
  // 1. Bulk Searchë¡œ keyword-based candidates
  const bulkResults = await this.searchPapers(query, { limit: 500 });

  // 2. Semantic Scholarì˜ Recommendations API ì‚¬ìš©
  // (ë‚´ë¶€ì ìœ¼ë¡œ SPECTER2 embeddings í™œìš©)
  const semanticResults = await this.getRecommendations(query, { limit: 500 });

  // 3. Merge & deduplicate
  const merged = this.mergeAndDeduplicate([...bulkResults, ...semanticResults]);

  // 4. Top 100-200 candidates ë°˜í™˜
  return merged.slice(0, 200);
}
```

#### Stage 2: Abstract ê¸°ë°˜ Re-ranking (ìƒˆë¡œ êµ¬í˜„ í•„ìš”)

```typescript
// src/lib/gemini/paper-selector.ts (ìƒˆ íŒŒì¼)
import { GoogleGenerativeAI } from '@google/generative-ai';

export class PaperSelector {
  private gemini: GoogleGenerativeAI;

  constructor(apiKey: string) {
    this.gemini = new GoogleGenerativeAI(apiKey);
  }

  /**
   * Geminië¡œ ë…¼ë¬¸ abstracts ë¶„ì„í•˜ì—¬ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
   */
  async rankPapersByRelevance(
    papers: Paper[],
    collectionTopic: string,
    topK: number = 50
  ): Promise<{ paper: Paper; score: number; reasoning: string }[]> {
    const model = this.gemini.getGenerativeModel({ model: 'gemini-2.5-flash' });

    // Batchë¡œ ì²˜ë¦¬ (í•œ ë²ˆì— 10-20ê°œì”©)
    const batchSize = 20;
    const results: Array<{ paper: Paper; score: number; reasoning: string }> =
      [];

    for (let i = 0; i < papers.length; i += batchSize) {
      const batch = papers.slice(i, i + batchSize);

      const prompt = `
You are a research paper relevance evaluator. Given a research topic and a list of paper abstracts,
rate how relevant each paper is to the topic on a scale of 0-10.

Research Topic: "${collectionTopic}"

Papers:
${batch
  .map(
    (p, idx) => `
[${idx + 1}] Title: ${p.title}
    Authors: ${p.authors?.map(a => a.name).join(', ')}
    Year: ${p.year}
    Citations: ${p.citationCount}
    Abstract: ${p.abstract || 'No abstract available'}
`
  )
  .join('\n')}

For each paper, provide:
1. Relevance score (0-10)
2. Brief reasoning (one sentence)

Respond in JSON format:
{
  "evaluations": [
    { "paperId": 1, "score": 8, "reasoning": "..." },
    ...
  ]
}
`;

      const result = await model.generateContent(prompt);
      const response = result.response.text();
      const evaluation = JSON.parse(response);

      // ê²°ê³¼ ì €ì¥
      evaluation.evaluations.forEach((e: any) => {
        results.push({
          paper: batch[e.paperId - 1],
          score: e.score,
          reasoning: e.reasoning,
        });
      });
    }

    // Scoreë¡œ ì •ë ¬í•˜ì—¬ Top-K ë°˜í™˜
    return results.sort((a, b) => b.score - a.score).slice(0, topK);
  }
}
```

**ì‚¬ìš© ì˜ˆì‹œ:**

```typescript
// src/lib/jobs/collection-builder.ts
import { PaperSelector } from '@/lib/gemini/paper-selector';

async function buildCollection(collectionId: string, topic: string) {
  // Stage 1: Initial search with Semantic Scholar
  const candidates = await semanticScholar.searchPapersWithEmbeddings(topic, {
    limit: 200,
  });

  console.log(`Found ${candidates.length} candidate papers`);

  // Stage 2: Re-rank with Gemini
  const selector = new PaperSelector(process.env.GEMINI_API_KEY!);
  const rankedPapers = await selector.rankPapersByRelevance(
    candidates,
    topic,
    50
  );

  console.log('Top 10 papers:');
  rankedPapers.slice(0, 10).forEach((r, idx) => {
    console.log(`${idx + 1}. [Score: ${r.score}] ${r.paper.title}`);
    console.log(`   Reasoning: ${r.reasoning}`);
  });

  // Stage 3: Download & Index only top 50 papers
  for (const { paper } of rankedPapers) {
    if (paper.openAccessPdf) {
      await downloadAndIndexPaper(collectionId, paper);
    }
  }
}
```

### 7.3 ë¹„ìš© ì ˆê° íš¨ê³¼

**Before (í˜„ì¬ ë°©ì‹):**

```
1. Semantic Scholar ê²€ìƒ‰: 1,000ê°œ í›„ë³´
2. ëª¨ë“  Open Access PDFs ë‹¤ìš´ë¡œë“œ: ~300ê°œ (30%)
3. Gemini File Searchë¡œ indexing: 300ê°œ
   - 300 papers Ã— 20K tokens = 6M tokens
   - Indexing cost: $0.15/1M Ã— 6 = $0.90
   - Storage: ~600MB (Free tier ì´ˆê³¼ ê°€ëŠ¥)
```

**After (Two-Stage):**

```
1. Semantic Scholar ê²€ìƒ‰: 200ê°œ í›„ë³´ (hybrid search)
2. Gemini Re-ranking: 200 â†’ 50
   - 200 abstracts Ã— 250 words = 50K tokens
   - Cost: $0.001 (ë¬´ì‹œ ê°€ëŠ¥)
3. Top 50ë§Œ PDF ë‹¤ìš´ë¡œë“œ & indexing
   - 50 papers Ã— 20K tokens = 1M tokens
   - Indexing cost: $0.15
   - Storage: ~100MB (Free tier ì¶©ë¶„)

ì ˆê°: $0.90 â†’ $0.15 (83% ê°ì†Œ)
```

### 7.4 UX ê°œì„ 

**Before:**

```
1. ì»¬ë ‰ì…˜ ìƒì„±
2. "ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..." (ë¡œë”©)
3. 30ë¶„ í›„: "300ê°œ ë…¼ë¬¸ ì¶”ê°€ ì™„ë£Œ"
4. ì‚¬ìš©ì: "ê´€ë ¨ ì—†ëŠ” ë…¼ë¬¸ë„ ë§ë„¤..."
```

**After:**

```
1. ì»¬ë ‰ì…˜ ìƒì„±
2. "ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..." (0.5ì´ˆ)
3. "200ê°œ í›„ë³´ ë°œê²¬, ê´€ë ¨ì„± ë¶„ì„ ì¤‘..."
4. ì§„í–‰ ìƒí™© í‘œì‹œ:
   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 60/200 papers analyzed

   Top candidates so far:
   1. [Score: 9.5] "Attention Is All You Need"
   2. [Score: 9.2] "BERT: Pre-training of Deep..."
   3. [Score: 8.8] "GPT-3: Language Models are Few-Shot..."

5. 30ì´ˆ í›„: "Top 50 ê´€ë ¨ ë…¼ë¬¸ ì„ ë³„ ì™„ë£Œ"
6. "PDF ë‹¤ìš´ë¡œë“œ ë° ì¸ë±ì‹± ì¤‘... 10/50 ì™„ë£Œ"
7. 10ë¶„ í›„: "ì»¬ë ‰ì…˜ ì¤€ë¹„ ì™„ë£Œ! ì±„íŒ…ì„ ì‹œì‘í•˜ì„¸ìš”."
```

### 7.5 êµ¬í˜„ ë¡œë“œë§µ

#### Phase 1: Abstract ê¸°ë°˜ Re-ranking (2ì£¼)

**Tasks:**

1. `PaperSelector` í´ë˜ìŠ¤ êµ¬í˜„
   - Gemini APIë¡œ abstract ë¶„ì„
   - Batch processing ì§€ì›
   - ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°

2. Collection Builder ìˆ˜ì •
   - Two-stage workflow í†µí•©
   - ì§„í–‰ ìƒí™© UI ì—…ë°ì´íŠ¸

3. E2E í…ŒìŠ¤íŠ¸
   - ë‹¤ì–‘í•œ ì£¼ì œë¡œ í…ŒìŠ¤íŠ¸
   - ë¹„ìš© ë° ì‹œê°„ ì¸¡ì •

**ì˜ˆìƒ íš¨ê³¼:**

- ë¹„ìš© 83% ì ˆê°
- ë…¼ë¬¸ í’ˆì§ˆ í–¥ìƒ (Precision 30% â†’ 85%)
- ì‚¬ìš©ì ë§Œì¡±ë„ ì¦ê°€

#### Phase 2: SPECTER2 Embeddings í™œìš© (1ì£¼)

**Tasks:**

1. Semantic Scholar Recommendations API í†µí•©
   - `/paper/{id}/recommendations` ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
   - Hybrid search (BM25 + SPECTER2)

2. Embedding ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰
   - Queryë¥¼ SPECTER2ë¡œ embedding
   - Cosine similarityë¡œ ranking

**ì˜ˆìƒ íš¨ê³¼:**

- Recall 95% â†’ 98% í–¥ìƒ
- ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë…¼ë¬¸ ë°œê²¬ (í‚¤ì›Œë“œ ë‹¤ë¥´ë”ë¼ë„)

#### Phase 3: Advanced Features (2ì£¼)

**Tasks:**

1. Citation/Reference ë„¤íŠ¸ì›Œí¬ í™œìš©
   - `/paper/{id}/citations` êµ¬í˜„
   - `/paper/{id}/references` êµ¬í˜„
   - "Expand Collection" ê¸°ëŠ¥ (ê´€ë ¨ ë…¼ë¬¸ ìë™ ì¶”ê°€)

2. Author-based Collections
   - `/author/search` êµ¬í˜„
   - `/author/{id}/papers` êµ¬í˜„
   - "íŠ¹ì • ì €ìì˜ ëª¨ë“  ë…¼ë¬¸" ì»¬ë ‰ì…˜

3. Snippet Search
   - `/snippet/search` êµ¬í˜„
   - Advanced Search UI
   - ë…¼ë¬¸ ë³¸ë¬¸ì—ì„œ íŠ¹ì • ê°œë… ê²€ìƒ‰

**ì˜ˆìƒ íš¨ê³¼:**

- ë…¼ë¬¸ íƒìƒ‰ ê²½í—˜ ëŒ€í­ í–¥ìƒ
- Power usersë¥¼ ìœ„í•œ ê³ ê¸‰ ê¸°ëŠ¥ ì œê³µ

---

## 8. ì°¸ê³  ìë£Œ

### 8.1 í•™ìˆ  ìë£Œ

**SPECTER & SPECTER2:**

- [SPECTER: Document-level Representation Learning using Citation-informed Transformers](https://arxiv.org/abs/2004.07180) (ACL 2020)
- [SPECTER2: Adapting scientific document embeddings to multiple fields and task formats](https://allenai.org/blog/specter2)
- [GitHub: allenai/specter](https://github.com/allenai/specter)
- [GitHub: allenai/specter2](https://github.com/allenai/specter2)
- [HuggingFace: allenai/specter2](https://huggingface.co/allenai/specter2)

**Two-Stage Retrieval & Hybrid Search:**

- [Deep Retrieval at CheckThat! 2025](https://arxiv.org/html/2505.23250v1) - Hybrid retrieval pipeline (BM25 + FAISS + LLM re-ranking)
- [Hybrid Search: Effectively Combining Keywords and Semantic Searches](https://www.semanticscholar.org/paper/Hybrid-Search%3A-Effectively-Combining-Keywords-and-Bhagdev-Chapman/adfc4e68d2e4e5c61f18608ae9f9cd830939fbdf)
- [Comprehensive review of academic search systems](https://link.springer.com/article/10.1007/s13278-025-01476-1) (2025)

**RAG & Chunking Strategies:**

- [A Guide to Chunking Strategies for RAG](https://zilliz.com/learn/guide-to-chunking-strategies-for-rag)
- [Chunking Strategies to Improve Your RAG Performance](https://weaviate.io/blog/chunking-strategies-for-rag)
- [Finding the Best Chunking Strategy for Accurate AI Responses](https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses) (NVIDIA)

### 8.2 API ë¬¸ì„œ

**Semantic Scholar API:**

- [Official API Documentation](https://api.semanticscholar.org/api-docs/)
- [API Tutorial](https://www.semanticscholar.org/product/api/tutorial)
- [Swagger/OpenAPI Spec](https://api.semanticscholar.org/graph/v1/swagger.json)
- [GitHub: paper-embedding-public-apis](https://github.com/allenai/paper-embedding-public-apis)
- [The Semantic Scholar Academic Graph (S2AG)](https://arxiv.org/html/2301.10140v2)

**Gemini File Search API:**

- [Official Documentation](https://ai.google.dev/gemini-api/docs/file-search)
- [Grounding with Google Search and your own data](https://ai.google.dev/gemini-api/docs/grounding)

### 8.3 Elicit ê´€ë ¨

**Official Resources:**

- [Elicit Homepage](https://elicit.com/)
- [Elicit Blog: Build a search engine, not a vector DB](https://elicit.com/blog/search-vs-vector-db/)
- [Elicit Support: Paper Sources](https://support.elicit.com/en/articles/553025)
- [Elicit Support: Workflow Options](https://support.elicit.com/en/articles/1418881)

**Reviews & Guides:**

- [How to Use Elicit: A Step-by-Step Guide in 2025](https://www.fahimai.com/how-to-use-elicit)
- [Systematic Literature Review with Elicit AI](https://medium.com/@borisnikolaev_57179/systematic-literature-review-with-elicit-ai-4-practical-use-cases-limitations-002e295caf41)
- [Elicit AI Review 2025: The Complete Guide](https://techfixai.com/elicit-ai-review/)

### 8.4 Tools & Libraries

**Vector Search:**

- [FAISS (Facebook AI Similarity Search)](https://github.com/facebookresearch/faiss)
- [Pinecone](https://www.pinecone.io/)
- [Weaviate](https://weaviate.io/)

**Embedding Models:**

- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [Gemini Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)
- [SentenceTransformers](https://www.sbert.net/)

---

## ìš”ì•½

### í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **Two-Stage Retrievalì€ í•„ìˆ˜**
   - Abstract ê¸°ë°˜ ì´ˆê¸° ê²€ìƒ‰ (ë¹ ë¥´ê³  ì €ë ´)
   - ì„ ë³„ëœ ë…¼ë¬¸ë§Œ Full-text ì²˜ë¦¬ (ì •í™•í•˜ì§€ë§Œ ë¹„ìŒˆ)

2. **Semantic Scholarì˜ ì¸í”„ë¼ í™œìš©**
   - 2ì–µ+ ë…¼ë¬¸ì˜ SPECTER2 embeddings ë¬´ë£Œ ì œê³µ
   - ì§ì ‘ embedding ì¸í”„ë¼ êµ¬ì¶• ë¶ˆí•„ìš”
   - APIë§Œìœ¼ë¡œ ì¶©ë¶„

3. **Abstractë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ íš¨ê³¼ì **
   - 95%+ Recall ë‹¬ì„± ê°€ëŠ¥
   - ë…¼ë¬¸ discovery ë‹¨ê³„ì— ìµœì 
   - Full-textëŠ” RAG ë‹¨ê³„ì—ì„œë§Œ í•„ìš”

4. **Hybrid Searchê°€ í•µì‹¬**
   - BM25 (keyword) + Semantic (embedding)
   - ë‘˜ì˜ ì¥ì ì„ ëª¨ë‘ í™œìš©
   - False negative ìµœì†Œí™”

### CiteBite ì•¡ì…˜ ì•„ì´í…œ

**ì¦‰ì‹œ ì ìš© ê°€ëŠ¥:**

1. âœ… Geminië¡œ Abstract ê¸°ë°˜ Re-ranking êµ¬í˜„ (2ì£¼)
2. âœ… Top 50 ë…¼ë¬¸ë§Œ indexing (ë¹„ìš© 83% ì ˆê°)
3. âœ… ì§„í–‰ ìƒí™© UI ê°œì„  (ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ)

**ì¤‘ê¸° ëª©í‘œ:** 4. â³ Semantic Scholar Recommendations API í†µí•© (1ì£¼) 5. â³ Hybrid Search êµ¬í˜„ (BM25 + SPECTER2) 6. â³ Citations/References ë„¤íŠ¸ì›Œí¬ í™œìš© (2ì£¼)

**ì¥ê¸° ë¹„ì „:** 7. ğŸ”® Author-based Collections 8. ğŸ”® Snippet Search (ë³¸ë¬¸ ê²€ìƒ‰) 9. ğŸ”® Citation ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-18
**ì‘ì„±ì**: Claude Code + User Discussion
**ê´€ë ¨ ë¬¸ì„œ**:

- [semantic-scholar-search-methods.md](./semantic-scholar-search-methods.md)
- [ROADMAP.md](../ROADMAP.md)
- [EXTERNAL_APIS.md](../planning/EXTERNAL_APIS.md)
